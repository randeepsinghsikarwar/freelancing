{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urG_6owfqwuS"
   },
   "source": [
    "## Part 2 - Dealing with Missing Data\n",
    "\n",
    "\n",
    "**Notice: This notebook is a modification of [sniff.ipynb](https://mlbook.explained.ai/notebooks/index.html) by Terence Parr and Jeremy Howard, which was used by permission of the author.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tcl3KFEXqwuT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from rfpimp_MC import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x13zTYO3qwuU"
   },
   "outputs": [],
   "source": [
    "def evaluate(X, y, n_estimators=50):\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, n_jobs=-1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "    oob = rf.oob_score_\n",
    "    n = rfnnodes(rf)\n",
    "    h = np.median(rfmaxdepths(rf))\n",
    "    print(f\"OOB R^2 is {oob:.5f} using {n:,d} tree nodes with {h} median tree depth\")\n",
    "    return rf, oob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj3GJ3jqqwuU"
   },
   "outputs": [],
   "source": [
    "def showimp(rf, X, y):\n",
    "    features = list(X.columns)\n",
    "    I = importances(rf, X, y, features=features)\n",
    "    plot_importances(I, color='#4575b4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_GlOGqhqwuW"
   },
   "outputs": [],
   "source": [
    "from pandas.api.types import is_string_dtype, is_object_dtype\n",
    "\n",
    "def df_normalize_strings(df):\n",
    "    for col in df.columns:\n",
    "        if is_string_dtype(df[col]) or is_object_dtype(df[col]):\n",
    "            df[col] = df[col].str.lower()\n",
    "            df[col] = df[col].fillna(np.nan)\n",
    "            df[col] = df[col].replace('none or unspecified', np.nan)\n",
    "            df[col] = df[col].replace('none', np.nan)\n",
    "            df[col] = df[col].replace('#name?', np.nan)\n",
    "            df[col] = df[col].replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddpVMLgsqwuX"
   },
   "outputs": [],
   "source": [
    "def extract_sizes(df, colname):\n",
    "    df[colname] = df[colname].str.extract(r'(\\d+\\.\\d+|\\d+)', expand=True)\n",
    "    df[colname] = df[colname].replace('', np.nan)\n",
    "    df[colname] = pd.to_numeric(df[colname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5O1yn2v7Lgq"
   },
   "outputs": [],
   "source": [
    "# modified version of: https://stackoverflow.com/questions/26986655/changing-height-feet-and-inches-to-an-integer-in-python-pandas\n",
    "\n",
    "def parse_length(length):\n",
    "    if not pd.isnull(length):\n",
    "      split_length = length.split(\"' \")\n",
    "      feet = float(split_length[0])\n",
    "      inches = float(split_length[1].replace(\"\\\"\",\"\"))\n",
    "      return (12*feet) + inches\n",
    "    else:\n",
    "      return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o_MHJ9LJtVO"
   },
   "source": [
    "### Recap\n",
    "\n",
    "It is a good idea to recap what we did to the data last time:\n",
    "\n",
    "- dropped the `SalesID` and `MachineID` features;\n",
    "- converted `auctioneerID` to 'string' data type so we can treat it as a categorical feature;\n",
    "- decided to leave `Blade_Width` as 'string' and treat as a categorical feature instead of converting to numeric;\n",
    "- extracted numeric features from the original `Undercarriage_Pad_Width` and `Tire_Size` strings;\n",
    "- converted `Stick_Length` to a numeric feature from the original string representation;\n",
    "- normalized the representation of missing values to `np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Our next steps will be to carry out the following:\n",
    "- convert all 'string' features to ordered categorical features;\n",
    "- label encode all these features using the value of 0 to represent missing data;\n",
    "- fix some remaining problems with numeric columns; and \n",
    "- replace missing numeric data by:\n",
    "    - adding a new feature to say whether or not that value was missing; \n",
    "    - replace missing values in the original feature with the median of all values for that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkxZmXgdqwuU"
   },
   "source": [
    "### Reset Data\n",
    "\n",
    "In this notebook we are going to pick up where we left off in **Part 1** so we'll load and process the data according to what we did in the last notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "l951ilWnqwuU",
    "outputId": "a5319b1e-95de-40d4-e71f-f003fb3cb64d"
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_feather(\"bulldozer-train.feather\")\n",
    "df = df_raw.copy()\n",
    "df = df.iloc[-100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['SalesID', 'MachineID'], axis=1, inplace=True)\n",
    "df['auctioneerID'] = df['auctioneerID'].astype(str)\n",
    "df_normalize_strings(df)\n",
    "extract_sizes(df, 'Tire_Size')\n",
    "extract_sizes(df, 'Undercarriage_Pad_Width')\n",
    "df['Stick_Length'] = df['Stick_Length'].apply(lambda x: parse_length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7F60ctRqwuV"
   },
   "outputs": [],
   "source": [
    "def sniff_modified(df):\n",
    "    with pd.option_context(\"display.max_colwidth\", 20):\n",
    "        info = pd.DataFrame()\n",
    "        info['data type'] = df.dtypes\n",
    "        info['percent missing'] = df.isnull().sum()*100/len(df)\n",
    "        info['No. unique'] = df.apply(lambda x: len(x.unique()))\n",
    "        info['unique values'] = df.apply(lambda x: x.unique())\n",
    "        return info.sort_values('data type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0uqYKR2qwuV"
   },
   "outputs": [],
   "source": [
    "sniff_modified(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPKsoTa3qwuW"
   },
   "source": [
    "### Handling Categorical Data\n",
    "\n",
    "For this part we are going to use some built functionality of Pandas, as opposed to the `catgory_encoders` package we used last time. To see how this is going to work, we'll carry out our procedure on a toy dataframe first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyd = pd.DataFrame({'Hydraulics_Flow': df['Hydraulics_Flow'].unique()})\n",
    "hyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the feature `Hydraulics_Flow`, which is a string feature, to a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyd['Hydraulics_Flow'] = hyd['Hydraulics_Flow'].astype('category').cat.as_ordered()\n",
    "hyd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we label encode the feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyd['default cat code'] = hyd['Hydraulics_Flow'].cat.codes\n",
    "hyd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we add 1 so that all missing values (`np.nan`) will be coded as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyd['our cat code'] = hyd['Hydraulics_Flow'].cat.codes + 1\n",
    "hyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we would do these two steps and replace the original feature with the encoded values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyd = pd.DataFrame({'Hydraulics_Flow': df['Hydraulics_Flow'].unique()})\n",
    "hyd['Hydraulics_Flow'] = hyd['Hydraulics_Flow'].astype('category').cat.as_ordered()\n",
    "hyd['Hydraulics_Flow'] = hyd['Hydraulics_Flow'].cat.codes + 1\n",
    "hyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have many string features that we would like to convert in this way, we will use functions to make applying this procedure to many features more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_categorical_dtype, is_string_dtype\n",
    "\n",
    "def df_string_to_cat(df):\n",
    "    for col in df.columns:\n",
    "        if is_object_dtype(df[col]) or is_string_dtype(df[col]):\n",
    "            df[col] = df[col].astype('category').cat.as_ordered()\n",
    "\n",
    "def df_cat_to_catcode(df):\n",
    "    for col in df.columns:\n",
    "        if isinstance(df[col].dtype, pd.CategoricalDtype):\n",
    "            df[col] = df[col].cat.codes + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert all string features to categorical features and encode them with the following two lines of code. Note that we have also dealt with all the missing values, as they are encoded with the value of 0 for every feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_string_to_cat(df)\n",
    "df_cat_to_catcode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sniff_modified(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have dealt with all of the categorical features and can now move on to dealing with missing values in the numeric features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice\n",
    "\n",
    "> **The unreasonable effectiveness of label encoding categorical variables**\n",
    "*You might be wondering why it's “legal” to convert all of those unordered (nominal) categorical variables to ordered integers. We know for sure that assuming an order between categories is wrong. The short answer is that RF models can still partition such converted categorical features in a way that is predictive, possibly at the cost of a more complex tree model. This is definitely not true for many models, such as linear regression models (which require so-called “dummy” boolean columns, one for each unique categorical value* - [that is, one hot encoding]). *In practice, we've found label encoding categorical variables surprisingly effective, even when it seems more advanced methods would work better.* (Jeremy Howard and Terence Parr, end of Section 7.5.1 of *Mechanics of Machine Learning*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values for Numeric Data\n",
    "\n",
    "Now that the categorical features have been encoded and missing values have been taken care of we need to address the missing values in the remaining numeric features: `Tire_Size`, `Undercarriage_Pad_Width`, `YearMade`, `Stick_Length`, and `MachineHoursCurrentMeter`. \n",
    "\n",
    "The recipe we are going to use here consists of two steps: \n",
    "- create a boolean column that has a `True` entry if that corresponds to a missing value and `False` otherwise; and,\n",
    "- fill in the missing values with the median value for that feature. \n",
    "\n",
    "To see how it will work in practice, let's try out our recipe on a toy dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy = pd.DataFrame(data={'YearMade':[1995,2001,np.nan]})\n",
    "df_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 1, we add a new boolean column to keep track of where the missing data was. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy['YearMade_na'] = df_toy['YearMade'].isnull()\n",
    "df_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in step 2, we replace the missing value with the mean value for that feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value = df_toy['YearMade'].median()\n",
    "df_toy['YearMade'] = df_toy['YearMade'].fillna(median_value)\n",
    "df_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a function to apply both of these steps to any given feature in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_num(df, colname):\n",
    "    df[colname+'_na'] = pd.isnull(df[colname])\n",
    "    df[colname] = df[colname].fillna(df[colname].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start replacing, let's asses the situation with the remaining numeric features that have missing values to see if there are any remaining problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tire_Size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Undercarriage_Pad_Width'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stick_Length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(df['MachineHoursCurrentMeter'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(df['YearMade'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `Tire_Size`, `Undercarriage_Pad_Width`, and `Stick_Length` are good to go so let's start with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_missing_num(df, 'Tire_Size')\n",
    "fix_missing_num(df, 'Undercarriage_Pad_Width')\n",
    "fix_missing_num(df, 'Stick_Length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check that the missing numbers are now gone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tire_Size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Undercarriage_Pad_Width'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stick_Length'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YearMade` and `MachineHoursCurrentMeter` have some potential issues that we'll need to explore: \n",
    "- `YearMade` problem 1: it's doubtful that any bulldozers were made in the year 1000 so we will need to treat these as missing values. In fact some of the other entries seem suspicious so we will use a cutoff year of 1950; any year before 1950 we will consider as missing. So for this feature, we will need to:\n",
    "    - replace all values below 1950 with `np.nan`; and then,\n",
    "    - replace those missing values with the median value;\n",
    "- `YearMade` problem 2: some of the bulldozers have sale dates that come before it was made. For these, we will:\n",
    "    - replace the `YearMade` value with the sale date;\n",
    "- `MachineHoursCurrentMeter`: some are listed as having been used for 0 hours; while this may indicate that they are new, the age of the bulldozers suggests that this is probably a missing value (or the owner did not want to put in the true value). For this we will need to:\n",
    "    - replace the value of 0 with `np.nan`; and then,\n",
    "    - replace the missing values with the median value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix `YearMade` first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['YearMade']<1950, 'YearMade'] = np.nan\n",
    "fix_missing_num(df, 'YearMade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.eval(\"saledate.dt.year < YearMade\"), 'YearMade'] = df['saledate'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fix `MachineHoursCurrentMeter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.eval(\"MachineHoursCurrentMeter==0\"), 'MachineHoursCurrentMeter'] = np.nan\n",
    "fix_missing_num(df, 'MachineHoursCurrentMeter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check that it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(df['MachineHoursCurrentMeter'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(df['YearMade'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the final cleaned data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sniff_modified(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything cleaned up except for `saledate`, which we'll tackle in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn-C-mzDqwuV"
   },
   "source": [
    "### Recall Our Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RS5Uw5xDqwuV"
   },
   "outputs": [],
   "source": [
    "basefeatures = ['SalesID', 'MachineID', 'ModelID',\n",
    "                'datasource', 'YearMade',\n",
    "                'auctioneerID', 'MachineHoursCurrentMeter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrnlwBaaqwuV"
   },
   "outputs": [],
   "source": [
    "df_baseline = df_raw.copy() \n",
    "df_baseline = df_baseline.iloc[-100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMrmCMBKqwuV"
   },
   "outputs": [],
   "source": [
    "X_baseline = df_baseline[basefeatures]\n",
    "y_baseline = df_baseline['SalePrice']\n",
    "\n",
    "X_baseline = X_baseline.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugQ21IyNqwuV"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_baseline, oob_baseline_initial = evaluate(X_baseline, y_baseline, n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Qu8oMAbqwuV"
   },
   "outputs": [],
   "source": [
    "showimp(rf_baseline, X_baseline, y_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn-C-mzDqwuV"
   },
   "source": [
    "### Train a New Model\n",
    "\n",
    "Now let's use our cleaned up data to train a new model and see if we have improved the performance compared to the baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = df.drop(['SalePrice','saledate'], axis=1) \n",
    "y = df['SalePrice']\n",
    "\n",
    "rf, oob_all = evaluate(X, y, n_estimators=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with the cleaned up features we get a nice increase in our OOB $R^2$ score. To see where else we may get an improvement, let's look at the feature importances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see what we should try next. There is not much left to do with `YearMade` but the other important features, like `Productsize`, `fiProductClassDesc`, `Enclosure`, `Hydraulics_Flow`, `fiSecondaryDesc`, etc deserve a closer look. Along with `saledate`, that is what we will do next\n",
    "\n",
    "Before we finish, we will save our cleaned data so we don't have to repeat the cleaning process as we explore how to further improve our model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.to_feather(\"bulldozer-train-clean.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Missing_Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
