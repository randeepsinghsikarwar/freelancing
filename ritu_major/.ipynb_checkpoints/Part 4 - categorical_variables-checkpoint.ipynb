{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Categorically Speaking\n",
    "\n",
    "**Notice**: This notebook is a modification of [cats.ipynb and targetencode.ipynb](https://mlbook.explained.ai/notebooks/index.html) by Terence Parr and Jeremy Howard, which were used by permission of the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reestablish Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6P6fbYCnN7vH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rfpimp_MC import *\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, y):\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "    oob = rf.oob_score_\n",
    "    n = rfnnodes(rf)\n",
    "    h = np.median(rfmaxdepths(rf))\n",
    "    print(f\"OOB R^2 is {oob:.5f} using {n:,d} tree nodes with {h} median tree depth\")\n",
    "    return rf, oob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showimp(rf, X, y):\n",
    "    features = list(X.columns)\n",
    "    features.remove('latitude')\n",
    "    features.remove('longitude')\n",
    "    features += [['latitude','longitude']]\n",
    "\n",
    "    I = importances(rf, X, y, features=features)\n",
    "    plot_importances(I, color='#4575b4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "dJ83sz4UN7vR",
    "outputId": "f604b55c-b267-42a2-b595-b460360679e0"
   },
   "outputs": [],
   "source": [
    "rent = pd.read_csv('rent.csv')\n",
    "\n",
    "rent_clean = rent[(rent['price'] > 1000) & (rent['price'] < 10000)]\n",
    "rent_clean = rent_clean[(rent_clean['longitude'] !=0) | (rent_clean['latitude']!=0)]\n",
    "rent_clean = rent_clean[(rent_clean['latitude']>40.55) &\n",
    "                        (rent_clean['latitude']<40.94) &\n",
    "                        (rent_clean['longitude']>-74.1) &\n",
    "                        (rent_clean['longitude']<-73.67)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfeatures = ['bathrooms', 'bedrooms', 'longitude', 'latitude']\n",
    "\n",
    "X = rent_clean[numfeatures]\n",
    "y = rent_clean['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, oob = evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Existing Features\n",
    "\n",
    "Another thing we can do is mix features that we have in our data to create new ones. To give an example of this, let's create a new feature that combines the number of bedrooms with the number of bathrooms. What we are interested in is the ratio of bedroooms to bathrooms as this may an important predictor as well. The reasoning is that the more bedrooms you have, presumably the more people, and the more people you have can determine how long you have to wait for a bathroom to get ready, say for work or school. So, maybe the ratio of bedrooms to bathrooms would be a good feature to have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_clean['beds_to_baths'] = rent_clean['bedrooms'] / (rent_clean['bathrooms'] + 1)\n",
    "rent_clean.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll see if this has any impact on our model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rent_clean[numfeatures + ['beds_to_baths']]\n",
    "y = rent_clean['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, oob = evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe total number of bedrooms plus bathrooms is what is important, as opposed to having them both listed separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_clean['total_rooms'] = rent_clean['bathrooms'] + rent_clean['bedrooms']\n",
    "rent_clean.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rent_clean[['longitude', 'latitude', 'total_rooms']]\n",
    "y = rent_clean['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, oob = evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, maybe not! \n",
    "\n",
    "We don't have a lot of raw numeric features to play around with here, but if you did, you would want to explore other combinations and measure their impact on how well the model generalizes. \n",
    "\n",
    "We do have another numeric variable to consider, which is the target, `price`. And yes, combining the target with other features is as dangerous as it sounds. Let's explore that next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target Encoding\n",
    "\n",
    "Any new feature that includes information about the target is referred to as *target encoding*. This is a technique that can sometimes be used to encode categorical variables. The simplest way to do this would be to use the mean of the target for each unique category value. Let's see how this would work for `building_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_clean[['building_id', 'price']].groupby('building_id').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above groups all the rows according to unique building_id values and then calculates the mean of all the apartments associated with that ID. We would now use the mean prices in place of building_id. (We will ignore  the funny looking `building_id = 0`, although we would normally inverstigate that.) \n",
    "\n",
    "**Warning**: Using target information in our features is prone to overfitting so it is usually best to use a library like `categorical_encoders` to do this, which is what we will do now. And, to be careful, we will use a validation set, instead of the out-of-bag score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rent_clean[numfeatures + ['building_id']]\n",
    "y = rent_clean['price']\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a baseline for the original 4 numeric features but using a validation set instead of the out-of-bag score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "X_train_orig = X_train[numfeatures]\n",
    "y_train_orig = y_train.copy()\n",
    "X_val_orig = X_val[numfeatures]\n",
    "y_val_orig = y_val.copy()\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "rf.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "val_score_orig = rf.score(X_val_orig, y_val_orig)\n",
    "\n",
    "print(f\"{val_score_orig:4f} score {rfnnodes(rf):,d} tree nodes and {np.median(rfmaxdepths(rf))} median tree height\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X_train_orig, y_train_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a validation score for a model that uses the target encoded `building_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.TargetEncoder(cols=['building_id'])\n",
    "\n",
    "encoder.fit(X_train, y_train)\n",
    "\n",
    "X_train_enc = encoder.transform(X_train, y_train)\n",
    "y_train_enc = y_train.copy()\n",
    "X_val_enc = encoder.transform(X_val)\n",
    "y_val_enc = y_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_enc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "rf.fit(X_train_enc, y_train_enc)\n",
    "\n",
    "val_enc_score = rf.score(X_val_enc, y_val_enc)\n",
    "\n",
    "print(f\"{val_enc_score:4f} score {rfnnodes(rf):,d} tree nodes and {np.median(rfmaxdepths(rf))} median tree height\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X_train_enc, y_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model feels that the target encoded `building_id` feature is, by a substantial margin, the most important feature. This is suspicious and indicates that the model is overfitting, that is, attaching too much importance to this feature as opposed to the other features that we know are important; most likely a result of using the target information. \n",
    "\n",
    "It may not be useful here, but it is good to know this technique and how to properly apply it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "Try *target encoding* the `manager_id` and `display_address` features and comparing the results to the baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Features Using Other Data\n",
    "\n",
    "After you have explored all you can do with the original raw data, it may be worthwhile to include information from other sources. For apartments in New York City, as is the case in most cities, adding a *neighbourhood* feature may have predictive power. While we have `latitude` and `longitude` different neighbourhoods may command different prices for an apartment with otherwise similar features because some areas of the city are more trendy than others; and, as a result, people may be willing to pay more to live there. (See [Chapter 6](https://mlbook.explained.ai/catvars.html#sec:6.6) for more details.)\n",
    "\n",
    "We can first using something like maps.google.com (similar) to estimate the latitude and longitude of some neighbourhoods in New York City and store that information in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoods = {\n",
    "    \"hells\" : [40.7622, -73.9924],\n",
    "    \"astoria\" : [40.7796684, -73.9215888],\n",
    "    \"Evillage\" : [40.723163774, -73.984829394],\n",
    "    \"Wvillage\" : [40.73578, -74.00357],\n",
    "    \"LowerEast\" : [40.715033, -73.9842724],\n",
    "    \"UpperEast\" : [40.768163594, -73.959329496],\n",
    "    \"ParkSlope\" : [40.672404, -73.977063],\n",
    "    \"Prospect Park\" : [40.93704, -74.17431],\n",
    "    \"Crown Heights\" : [40.657830702, -73.940162906],\n",
    "    \"financial\" : [40.703830518, -74.005666644],\n",
    "    \"brooklynheights\" : [40.7022621909, -73.9871760513],\n",
    "    \"gowanus\" : [40.673, -73.997]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to create a new feature for each neighbourhood that calculates the, appropriately named, *Manhattan distance* from each apartment to that neighbourhood. We use Manhattan distance as that could be considered more connected to walking distance from the apartment to the neighbourhood in question. \n",
    "\n",
    "\n",
    "<img src=\"manhattan_dist.jpg\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hood, loc in hoods.items():\n",
    "    rent_clean[hood] = np.abs(rent_clean['latitude'] - loc[0]) + np.abs(rent_clean['longitude'] - loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_clean[numfeatures + list(hoods.keys())].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see if this helps out our model at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rent_clean[numfeatures + list(hoods.keys())]\n",
    "y = rent_clean['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, oob = evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could probably remove the original `latitude` and `longitude` columns as there is now enough *location* information contained within the neghbourhood data, but we'll leave them all in here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- Try combining all the features we have created in **Part 1**, **Part 2**, **Part 3**, and **Part 4** of our categorical variables notebooks. Build and evaluate a model against the baseline, being sure to look at the feature importances. \n",
    "- Explore the data contained in *adult.csv* using some of the techniques you now know about:\n",
    "    - Note that this is a classification problem; and \n",
    "    - You should do label encoding on the target as your first step. The easiest way to do this is to use the [Pandas `.map()` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "new_initial_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
