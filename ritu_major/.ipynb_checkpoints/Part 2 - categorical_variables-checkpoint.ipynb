{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Categorically Speaking\n",
    "\n",
    "**Notice**: This notebook is a modification of [cats.ipynb and targetencode.ipynb](https://mlbook.explained.ai/notebooks/index.html) by Terence Parr and Jeremy Howard, which were used by permission of the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reestablish Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6P6fbYCnN7vH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rfpimp_MC import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, y):\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "    oob = rf.oob_score_\n",
    "    n = rfnnodes(rf)\n",
    "    h = np.median(rfmaxdepths(rf))\n",
    "    print(f\"OOB R^2 is {oob:.5f} using {n:,d} tree nodes with {h} median tree depth\")\n",
    "    return rf, oob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showimp(rf, X, y):\n",
    "    features = list(X.columns)\n",
    "    features.remove('latitude')\n",
    "    features.remove('longitude')\n",
    "    features += [['latitude','longitude']]\n",
    "\n",
    "    I = importances(rf, X, y, features=features)\n",
    "    plot_importances(I, color='#4575b4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "dJ83sz4UN7vR",
    "outputId": "f604b55c-b267-42a2-b595-b460360679e0"
   },
   "outputs": [],
   "source": [
    "rent = pd.read_csv('rent.csv')\n",
    "\n",
    "rent_clean = rent[(rent['price'] > 1000) & (rent['price'] < 10000)]\n",
    "rent_clean = rent_clean[(rent_clean['longitude'] !=0) | (rent_clean['latitude']!=0)]\n",
    "rent_clean = rent_clean[(rent_clean['latitude']>40.55) &\n",
    "                        (rent_clean['latitude']<40.94) &\n",
    "                        (rent_clean['longitude']>-74.1) &\n",
    "                        (rent_clean['longitude']<-73.67)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numfeatures = ['bathrooms', 'bedrooms', 'longitude', 'latitude']\n",
    "\n",
    "X = rent_clean[numfeatures]\n",
    "y = rent_clean['price']\n",
    "\n",
    "rf, oob = evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables\n",
    "\n",
    "Recall that categorical variables tend to come in two kinds:\n",
    "- Ordinal\n",
    "    - where there is some natural ordering to the categories\n",
    "- Nominal\n",
    "    - where there is no natural ordering to the categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Have We Ignored So Far?\n",
    "\n",
    "Our baseline model used the numeric features `bathrooms`, `bedrooms`, `longtitude`, and `latitutde`, but we know that our data has other features. We would now like to see if these features contain any information that our model can use; that is, to help the model better understand the relationship between the features and the target so as to improve its ability to make accurate predictions. \n",
    "\n",
    "Let's first look at the remaining features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnum = [col for col in rent_clean.columns.tolist() if col not in numfeatures + ['price']]\n",
    "nonnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_nonnum = rent_clean[nonnum]\n",
    "rent_nonnum.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows and columns do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_nonnum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each non-numeric column, let's find out how many unique values (that is, potential *categories*) there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_nonnum.nunique(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few things to notice are: \n",
    "- `interest_level` has only 3 categories\n",
    "- all the other features potentially have 1000's of categories\n",
    "- `listing_id` is a unique number per apartment so contains no information \n",
    "- `created` is a date which has almost as many values as apartments \n",
    "\n",
    "So, we'll ignore `created` and `listing_id` and we'll start with the simplest feature, `interest_level`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Let's take a moment to remember what we are doing. In order to train a model we need numeric data. Our data however, has many features that may contain information about our target `price` but are not numeric. So, our objective now is to see if we can convert these features into some sort of sensible numeric representation that may improve the performance of our baseline model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before We Begin\n",
    "\n",
    "Before we start tackling the categorical features in our data, we will need to install another package, called [category_encoders](https://contrib.scikit-learn.org/category_encoders/index.html). Much of this can be done in *sklearn*, but that is not always straightforward, or with straight python. To streamline our approach, we will use this new package so that we have a single interface to deal with. This package also integrates well with *sklearn*, *Pandas*, and *NumPy*. (Note: you will see some *FutureWarning* messages when using this package but these can be ignored just like we did for `rfpimp`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install category_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Ordinal Variable: `interest_level`\n",
    "\n",
    "This feature is an example of an *ordinal* categorical variable, which means there is a natural ordering. To see this, let's look at the unique values contained in this feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_clean['interest_level'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the obvious ordering is \n",
    "\n",
    "$$\n",
    "\\rm{low} \\rightarrow \\rm{medium} \\rightarrow \\rm{high}\n",
    "$$\n",
    "\n",
    "Now let's encode this feature using integers, making sure we maintain this ordering.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first add the `interest_level` feature to our numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rent_clean[numfeatures + ['interest_level']]\n",
    "y = rent_clean['price']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then encode this feature numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.OrdinalEncoder(mapping=[{'col': 'interest_level', 'mapping': {'low': 1, 'medium': 2, 'high':3}}])\n",
    "\n",
    "encoder.fit(X)\n",
    "X = encoder.transform(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach called *ordinal encoding*. Note that the scale doesn't matter for random forests (although it may for another model), so it wouldn't matter if we used 1, 2, 3 or 10, 20, 30 to encode this variable, as long as we maintained the original ordering. *Label encoding* is similar, but just assigns an integer to each unique category value (assuming no order) and, as the name suggests, is often used to numerically encode target classes. \n",
    "\n",
    "Now that the feature is numeric, we can include it in our baseline model and then evaluate how it performs to see if including `interest_level` improves performance at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, oob = evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that including this feature gives us a slight increase in performance since our OOB $R^2$ is now about 0.870 (up from 0.867). From the number of nodes in the forest we see that the model has to work a little harder when we include this feature. And we can also see that predictions will be a bit slower as the median depth of trees in the forest has increased from 35 to 36. \n",
    "\n",
    "But, the closer we get to $R^2 = 1$ the harder it is to get an improvement so we will keep this feature in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Nominal Variable: `manager_id`\n",
    "\n",
    "This type of categorical variable is more difficult to encode than an ordinal variable as there is often no meaningful way to encode as a number, especially when there are many category values. \n",
    "\n",
    "To get started, let's take a look at some typical values for this variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_clean['manager_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And determine how many unique values there are; that is, how many category values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rent_clean['manager_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a *high-cardinality* categorical variable as it has many category values. For instance, compare this number to the number of category values we had to deal with for `interest_level`. \n",
    "\n",
    "The first thing we are going to try is to ignore that this is a *nominal* variable and treat it as if it were a *ordinal* one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rent_clean[numfeatures + ['manager_id']]\n",
    "y = rent_clean['price']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OrdinalEncoder(cols=['manager_id'])\n",
    "\n",
    "encoder.fit(X)\n",
    "X = encoder.transform(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, oob = evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't improve our baseline $R^2$ performance and also adds many more nodes to the forest and about 5 levels to the median depth. So, this attempt doesn't work. \n",
    "\n",
    "We will now try a different method to encode this variable to see if that makes a difference: *frequency encoding*. In this approach we count up how many samples belong to each category and then replace each unique string value with the corresponding count. Let's see how this works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rent_clean[numfeatures + ['manager_id']]\n",
    "y = rent_clean['price']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.CountEncoder(cols=['manager_id'])\n",
    "\n",
    "encoder.fit(X)\n",
    "X = encoder.transform(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, oob = evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that including this feature with this encoding doesn't improve our overall results over our baseline. \n",
    "\n",
    "High-cardinality variables tend to require more advanced encodings, like *embeddings* and *target encoding*. We will revisit this feature later, when we talk about target encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Aside: One-Hot Encoding\n",
    "\n",
    "Another common technique for encoding nominal variables is called *one-hot encoding*. We have already seen this approach in Lab 1, and although this approach is not useful for high-cardinality variables, it is a good time to review how it works. To show how it works we will use a toy dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'manager_id':['mgr 1', 'mgr 1', 'mgr 3', 'mgr 1', 'mgr 2', 'mgr 2'], \n",
    "                   'bedrooms':[1, 2, 1, 1, 3, 2], \n",
    "                   'price':[3590, 2780, 6450, 4100, 3900, 4200]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we perform one-hot encoding on our pretend `manager_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OneHotEncoder(cols=['manager_id'])\n",
    "\n",
    "encoder.fit(df)\n",
    "df = encoder.transform(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note that we had 3 unique `manager_id` values in our toy dataset. When we do one-hot encoding on this data we will create 3 columns from the original `manager_id` column, one for each unique value. Then, we put a 1 (where one-hot comes from) in the column corresponding to the correct manager id for that sample (row) and 0's in the other two columns. If the number of unique values for the feature were 5 we would create 5 new columns instead of 3, as we did here, so this approach can be taken for any number of unique category values. \n",
    "\n",
    "However, note that if we had done this for the `manager_id` in our rent dataset, we would introduce 3409 new columns to our dataframe, which is impractical; that's too much computational overhead for a single feature. And, `manager_id`is not the only high-cardinality variable in our dataset. If we were to one-hot encode `manager_id`, `display_address`, and `building_id` we would add $3409 + 8692 + 7417 = 19518$ new columns to our data (see `rent_nonnum.nunique(dropna=False)` code cell above to see where these numbers came from). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the procedures we took for `manager_id` but now explore the effect of including `building_id` and `display_address`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "new_initial_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
