{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Categorically Speaking\n",
    "\n",
    "**Notice**: This notebook is a modification of [cats.ipynb and targetencode.ipynb](https://mlbook.explained.ai/notebooks/index.html) by Terence Parr and Jeremy Howard, which were used by permission of the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HsaH_N8N7vG"
   },
   "source": [
    "## Recap\n",
    "\n",
    "To train a model we need:\n",
    "\n",
    " - all the data to be numeric;\n",
    " - no missing data/values.\n",
    " \n",
    "And what we have done so far is:\n",
    " - ignored non-numeric data;\n",
    " - built and evaluated a random forest model, which had:\n",
    "     - a poor avg $R^2\\,$ and *mean absolute error* on the validation data;\n",
    "     - high variance in $R^2\\,$ and *mean absolute error* on the validation data;\n",
    " - explored our data for anomalies in the context of our objective to predict apartment rental prices for a typical apartment in New York City;\n",
    " - cleaned our data to remove the anomalies we discovered;\n",
    " - built and evaluated a random forest model using the cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reestablish Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6P6fbYCnN7vH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-bag R^2 for baseline model is: 0.051345739046082084\n"
     ]
    }
   ],
   "source": [
    "rent = pd.read_csv('rent.csv')\n",
    "\n",
    "numfeatures = ['bathrooms', 'bedrooms', 'longitude', 'latitude']\n",
    "\n",
    "X = rent[numfeatures]\n",
    "y = rent['price']\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "rf.fit(X, y)\n",
    "\n",
    "oob_noisy = rf.oob_score_\n",
    "print(f\"Out-of-bag R^2 for baseline model is: {oob_noisy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation R^2 for noisy model is: 0.03195870870926465\n",
      "Validation R^2 for noisy model is: 0.8625596483548396\n",
      "Validation R^2 for noisy model is: 0.9382727814644269\n",
      "Validation R^2 for noisy model is: 0.8019069969787642\n",
      "Validation R^2 for noisy model is: 0.8635663461946572\n",
      "Validation R^2 for noisy model is: 0.8269381208400364\n",
      "Validation R^2 for noisy model is: 0.8844483752761421\n",
      "Validation R^2 for noisy model is: 0.9856243585797346\n",
      "Validation R^2 for noisy model is: -4.061873479328193\n",
      "Validation R^2 for noisy model is: 0.8881302455281123\n",
      "Validation R^2 for noisy model is: 0.9040709538550382\n",
      "Validation R^2 for noisy model is: 0.9906608792025959\n",
      "Validation R^2 for noisy model is: 0.8338491534500002\n",
      "Validation R^2 for noisy model is: 0.8074717585113296\n",
      "Validation R^2 for noisy model is: 0.9748838361235869\n",
      "Validation R^2 for noisy model is: 0.9780104337007731\n",
      "Validation R^2 for noisy model is: 0.801748941649217\n",
      "Validation R^2 for noisy model is: 0.8176079052364837\n",
      "Validation R^2 for noisy model is: 0.9942970287999991\n",
      "Validation R^2 for noisy model is: 0.907095932497932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for i in range(20):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    val_score = rf.score(X_test, y_test)\n",
    "    print(f\"Validation R^2 for noisy model is: {val_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "dJ83sz4UN7vR",
    "outputId": "f604b55c-b267-42a2-b595-b460360679e0"
   },
   "outputs": [],
   "source": [
    "rent = pd.read_csv('rent.csv')\n",
    "\n",
    "rent_clean = rent[(rent['price'] > 1000) & (rent['price'] < 10000)]\n",
    "rent_clean = rent_clean[(rent_clean['longitude'] !=0) | (rent_clean['latitude']!=0)]\n",
    "rent_clean = rent_clean[(rent_clean['latitude']>40.55) &\n",
    "                        (rent_clean['latitude']<40.94) &\n",
    "                        (rent_clean['longitude']>-74.1) &\n",
    "                        (rent_clean['longitude']<-73.67)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-bag R^2 for baseline model is: 0.8680051027195075\n",
      "Out-of-bag R^2 for baseline model is: 0.8678009108868311\n",
      "Out-of-bag R^2 for baseline model is: 0.8669448115889948\n",
      "Out-of-bag R^2 for baseline model is: 0.8672657944975786\n",
      "Out-of-bag R^2 for baseline model is: 0.8687651894828148\n",
      "Out-of-bag R^2 for baseline model is: 0.8674891364203413\n",
      "Out-of-bag R^2 for baseline model is: 0.8681466598575913\n",
      "Out-of-bag R^2 for baseline model is: 0.8675027359911786\n",
      "Out-of-bag R^2 for baseline model is: 0.8677235575230293\n",
      "Out-of-bag R^2 for baseline model is: 0.8679393696474594\n",
      "Out-of-bag R^2 for baseline model is: 0.8681246211811058\n",
      "Out-of-bag R^2 for baseline model is: 0.867564689027841\n",
      "Out-of-bag R^2 for baseline model is: 0.8674369195510829\n",
      "Out-of-bag R^2 for baseline model is: 0.8671392646294537\n",
      "Out-of-bag R^2 for baseline model is: 0.8677955391958313\n",
      "Out-of-bag R^2 for baseline model is: 0.8683588218462751\n",
      "Out-of-bag R^2 for baseline model is: 0.8680161399084121\n",
      "Out-of-bag R^2 for baseline model is: 0.867465850974249\n",
      "Out-of-bag R^2 for baseline model is: 0.8676978113258889\n",
      "Out-of-bag R^2 for baseline model is: 0.8679751561328992\n"
     ]
    }
   ],
   "source": [
    "numfeatures = ['bathrooms', 'bedrooms', 'longitude', 'latitude']\n",
    "\n",
    "X = rent_clean[numfeatures]\n",
    "y = rent_clean['price']\n",
    "\n",
    "for i in range(20):\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    oob_baseline = rf.oob_score_\n",
    "    print(f\"Out-of-bag R^2 for baseline model is: {oob_baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$ Reminder\n",
    "\n",
    "Recall the formula:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "This tells us that:\n",
    "- $R^2 = 1$ means our model is perfect; \n",
    "- $R^2 \\approx 0$ means our model does no better than just predicting the average;\n",
    "- $R^2 \\lt\\lt 0$ means our model does worse than predicting the average.\n",
    "\n",
    "Also, as $R^2 \\rightarrow 1$ it gets harder and harder to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Indicators\n",
    "\n",
    "We are not only interested in $R^2$. We would also like to know: \n",
    "- how much work the random forest model has to do to capture the relationship between the features and the target; \n",
    "- the typical tree depth, as this will impact the speed of predictions for new data;\n",
    "- how important different features are for a given model.\n",
    "\n",
    "To help with this we will use the `rfpimp` package. (Note: you will see some *FutureWarning* messages when using this package but these can be ignored as they are just warnings that some parts of the *sklearn* code are changing in the future.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfpimp_MC import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be evaluating many models this way, we will use some functions to help keep our code clean:\n",
    "- one to evaluate our model and report the OOB score, the number of nodes across all trees in the forest, and the median tree depth in the forest; and, \n",
    "- one to show the feature importances for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, y):\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "    oob = rf.oob_score_\n",
    "    n = rfnnodes(rf)\n",
    "    h = np.median(rfmaxdepths(rf))\n",
    "    print(f\"OOB R^2 is {oob:.5f} using {n:,d} tree nodes with {h} median tree depth\")\n",
    "    return rf, oob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showimp(rf, X, y):\n",
    "    features = list(X.columns)\n",
    "    features.remove('latitude')\n",
    "    features.remove('longitude')\n",
    "    features += [['latitude','longitude']]\n",
    "\n",
    "    I = importances(rf, X, y, features=features)\n",
    "    plot_importances(I, color='#4575b4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try both of these out on our baseline model that uses only the cleaned numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Many times, a model's ability to generalize (predict) well is not all we are hoping for; we would also like to understand what the model is doing, which is referred to as a model's interpretability. Random Forests have this as a built in feature, however the implementation in *sklearn* suffers from bias when:\n",
    "- the scales of the features vary; and/or, \n",
    "- there are many categories for a feature.\n",
    "\n",
    "A better way to assess feature importance, in any model, is to use:\n",
    "- permutation importance; or \n",
    "- dropped feature importance.\n",
    "\n",
    "##### Permutation Importance\n",
    "\n",
    "We can calculate the feature importances using a permutation method, which consists of the following steps:\n",
    "- use all features and establish a baseline value for $R^2$;\n",
    "- select one feature and randomly permute its values leaving all other features unchanged;\n",
    "- calculate the new value for $R^2$ with this one feature permuted;\n",
    "- calculate the change in $R^2$ from the baseline; and, \n",
    "- repeat for the other features.\n",
    "\n",
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_importances(X, y):\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True, random_state=999)\n",
    "    rf.fit(X, y)\n",
    "    r2 = rf.oob_score_\n",
    "    print(f\"Baseline R^2 with no columns permuted: {r2:.5f}\\n\")\n",
    "    for col in X.columns:\n",
    "        X_col = X.copy()\n",
    "        X_col[col] = X_col[col].sample(frac=1).values\n",
    "        rf.fit(X_col, y)\n",
    "        r2_col = rf.oob_score_\n",
    "        print(f\"Permuting column {col}: new R^2 is {r2_col:.5f} and difference from baseline is {r2 - r2_col:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_importances(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropped Column Importance\n",
    "\n",
    "We can also calculate the importance of the features using a dropped column, which consists of the following steps:\n",
    "- use all features and establish a baseline value for $R^2$;\n",
    "- select one feature and remove it from the data;\n",
    "- calculate the new value for $R^2$ with this one feature removed;\n",
    "- calculate the change in $R^2$ from the baseline; and, \n",
    "- repeat for the other features.\n",
    "\n",
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_importances(X, y):\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True, random_state=999)\n",
    "    rf.fit(X, y)\n",
    "    r2 = rf.oob_score_\n",
    "    print(f\"Baseline R^2 with no columns dropped: {r2:.5f}\\n\")\n",
    "    for col in X.columns:\n",
    "        X_col = X.copy()\n",
    "        X_col = X_col.drop(col, axis=1) \n",
    "        rf.fit(X_col, y)\n",
    "        r2_col = rf.oob_score_\n",
    "        print(f\"Dropping column {col}: new R^2 is {r2_col:.5f} and difference from baseline is {r2 - r2_col:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_importances(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Be Careful With Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dup = X.copy()\n",
    "X_dup['bedrooms_dup'] = X_dup['bedrooms']\n",
    "X_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_importances(X_dup, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Breaking Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 2, X_dup.shape[0])\n",
    "\n",
    "X_dup['bedrooms_dup'] = X_dup['bedrooms'] + noise\n",
    "X_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_importances(X_dup, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "new_initial_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
